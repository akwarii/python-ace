#!/usr/bin/env python
import logging
import os
import sys
from argparse import ArgumentParser

import numpy as np

from pyace import ACEBBasisSet, BBasisConfiguration, aseatoms_to_atomicenvironment
from pyace.aceselect import (
    compute_batch_size,
    compute_mem_limit,
    compute_required_memory,
    load_datasets,
)
from pyace.activelearning import (
    compute_A_active_inverse,
    compute_active_set,
    compute_active_set_by_batches,
    compute_B_projections,
    compute_extrapolation_grade,
    compute_number_of_functions,
    count_number_total_atoms_per_species_type,
    extract_reference_forces_dict,
    save_active_inverse_set,
)
from pyace.preparedata import sizeof_fmt

LOG_FMT = "%(asctime)s %(levelname).1s - %(message)s"
logging.basicConfig(level=logging.INFO, format=LOG_FMT, datefmt="%Y/%m/%d %H:%M:%S")
log = logging.getLogger()


def build_parser() -> ArgumentParser:
    parser = ArgumentParser(
        prog="pace_activeset",
        description="Utility to compute the active set for PACE (.yaml) potential files.",
        add_help=False,
    )
    required = parser.add_argument_group("required named arguments")
    optional = parser.add_argument_group("optional named arguments")
    parser.add_argument(
        "potential_file",
        help="Path to the B-basis file (.yaml).",
        type=str,
    )
    required.add_argument(
        "-d",
        "--dataset",
        type=str,
        nargs="+",
        required=True,
        help="Path(s) to dataset file(s). Example: -d filename.pckl.gzip [filename2.pckl.gzip].",
    )
    optional.add_argument(
        "-h",
        "--help",
        action="help",
        help="Show this help message and exit",
    )
    optional.add_argument(
        "-q",
        "--query",
        type=str,
        default=None,
        help="Pandas .query argument for further filtering the dataset. Example: -q 'not name.str.contains(\"augmented\")'.",
    )
    optional.add_argument(
        "-f",
        "--full",
        help="Compute the active set on the full (linearized) design matrix.",
        action="store_true",
    )
    optional.add_argument(
        "-b",
        "--batch_size",
        help="Batch size (number of structures) to be considered simultaneously. If not provided, the entire dataset is considered at once.",
        default="auto",
        type=str,
    )
    optional.add_argument(
        "-g",
        "--gamma_tolerance",
        help="Tolerance for the gamma parameter.",
        default=1.01,
        type=float,
    )
    optional.add_argument(
        "-i",
        "--maxvol_iters",
        help="Maximum number of iterations for the MaxVol algorithm.",
        default=300,
        type=int,
    )
    optional.add_argument(
        "-r",
        "--maxvol_refinement",
        help="Number of refinement epochs.",
        default=5,
        type=int,
    )
    optional.add_argument(
        "-m",
        "--memory-limit",
        help="Memory limit (e.g., 1GB, 500MB, or 'auto').",
        default="auto",
        type=str,
    )
    optional.add_argument(
        "-e",
        "--error-force-threshold",
        default=0,
        type=float,
        help="Include only atoms with force RMSE error lower than the provided threshold in the active set. If zero, the upper range Q3+1.5*IQR will be used.",
        dest="error_based_threshold",
    )
    optional.add_argument(
        "-a",
        "--all-atoms",
        help="Consider all atoms for constructing the active set.",
        dest="all_atoms",
        default=False,
        action="store_true",
    )
    optional.add_argument(
        "-V",
        help="Suppress verbosity of numerical procedures.",
        dest="not_verbose",
        default=False,
        action="store_true",
    )

    return parser


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    potential_file = args.potential_file
    dataset_filename = args.dataset
    batch_size_option = args.batch_size
    gamma_tolerance = args.gamma_tolerance
    maxvol_iters = args.maxvol_iters
    maxvol_refinement = args.maxvol_refinement
    mem_lim_option = args.memory_limit
    is_full = args.full
    error_based_threshold = args.error_based_threshold
    all_atoms = args.all_atoms
    verbose = not args.not_verbose
    query = args.query

    mem_lim = compute_mem_limit(mem_lim_option)

    data_path = os.environ.get("PACEMAKERDATAPATH", "")
    if data_path:
        if verbose:
            log.info(f"Data path set to $PACEMAKERDATAPATH = {data_path}")

    # TODO when concatenating datasets, we need to ensure that the same columns are present in all datasets
    # if not, we should raise an error if necessary data are missing or generate mock data for columns such as name
    if isinstance(dataset_filename, list):
        print(dataset_filename, args.dataset)
        df = load_datasets(dataset_filename, data_path, verbose)
    else:
        raise ValueError(f"Unrecognized --dataset (-d) argument: {dataset_filename}")

    if verbose:
        log.info(f"Total number of structures: {len(df)}")

    if query is None and "name" in df.columns:
        size_before = len(df)
        df = df.query("not name.str.startswith('augmented')").reset_index(drop=True)
        ndrop = size_before - len(df)
        if verbose and ndrop > 0:
            log.info(
                f"AUGMENTED STRUCTURES: {ndrop} structure(s) were dropped, new dataset size: {len(df)}"
            )
    if query is not None and not all_atoms:
        if verbose:
            log.info(f"Applying query ``{query}``")
        df = df.query(query).reset_index(drop=True)
        if verbose:
            log.info(f"Total number of structures after query: {len(df)}")
    if verbose:
        log.info(f"Potential file: {potential_file}")

    bconf = BBasisConfiguration(potential_file)

    bbasis = ACEBBasisSet(bconf)
    nfuncs = compute_number_of_functions(bbasis)
    if is_full:
        n_projections = [
            p * bbasis.map_embedding_specifications[st].ndensity for st, p in enumerate(nfuncs)
        ]
    else:  # linear
        n_projections = nfuncs

    elements_to_index_map = bbasis.elements_to_index_map
    elements_name = bbasis.elements_name
    cutoffmax = bbasis.cutoffmax

    ATOMIC_ENV_COLUMN = "atomic_env"

    rebuild_atomic_env = False
    if ATOMIC_ENV_COLUMN not in df.columns:
        rebuild_atomic_env = True
    else:
        # check if cutoff is not smaller than requested now
        try:
            metadata_kwargs = df.metadata_dict[ATOMIC_ENV_COLUMN + "_kwargs"]
            metadata_cutoff = metadata_kwargs["cutoff"]
            if metadata_cutoff < cutoffmax:
                if verbose:
                    log.warning(
                        "WARNING! Column {} was constructed with smaller cutoff ({}A) "
                        "that necessary now ({}A). "
                        "Neighbourlists will be re-built".format(
                            ATOMIC_ENV_COLUMN, metadata_cutoff, cutoffmax
                        )
                    )
                rebuild_atomic_env = True
            else:
                if verbose:
                    log.info(
                        "Column '{}': existing cutoff ({}A) >= "
                        "requested  cutoff ({}A), skipping...".format(
                            ATOMIC_ENV_COLUMN, metadata_cutoff, cutoffmax
                        )
                    )
                rebuild_atomic_env = False

        except KeyboardInterrupt as e:
            raise e
        except Exception as e:
            if verbose:
                log.info(
                    "Could not extract cutoff metadata "
                    "for column '{}' (error: {}). Please ensure the valid cutoff for "
                    "precomputed neighbourlists".format(ATOMIC_ENV_COLUMN, e)
                )
            rebuild_atomic_env = True

    if rebuild_atomic_env:
        if verbose:
            log.info(
                "Constructing {} column, cutoffmax={}, elements_to_index_map={}".format(
                    ATOMIC_ENV_COLUMN, cutoffmax, elements_to_index_map
                )
            )
        df[ATOMIC_ENV_COLUMN] = df["ase_atoms"].apply(
            aseatoms_to_atomicenvironment,
            cutoff=cutoffmax,
            elements_mapper_dict=elements_to_index_map,
        )

    atomic_env_list = df[ATOMIC_ENV_COLUMN]
    structure_ind_list = df.index
    total_number_of_atoms_per_species_type = count_number_total_atoms_per_species_type(
        atomic_env_list
    )

    required_active_set_memory, required_projections_memory = compute_required_memory(
        total_number_of_atoms_per_species_type,
        elements_name,
        nfuncs,
        n_projections,
        verbose,
    )

    if verbose:
        log.info(
            "Required memory to store complete dataset projections: {}".format(
                sizeof_fmt(required_projections_memory)
            )
        )
        log.info(
            "Required memory to store active set: {}".format(
                sizeof_fmt(required_active_set_memory)
            )
        )
    num_structures = len(atomic_env_list)
    batch_size = compute_batch_size(
        batch_size_option,
        mem_lim,
        num_structures,
        required_active_set_memory,
        required_projections_memory,
        verbose,
    )

    if is_full:
        active_set_inv_filename = potential_file.replace(".yaml", ".asi.nonlinear")
        if verbose:
            log.info("FULL (non-linear) matrix will be used for active set calculation")
    else:
        active_set_inv_filename = potential_file.replace(".yaml", ".asi")
        if verbose:
            log.info("LINEAR matrix will be used for active set calculation")

    if batch_size is None:
        # single shot MaxVol
        if verbose:
            log.info("Single-run (no batch_size is provided)")
            log.info("Compute B-projections")
        A0_proj_dict, forces_dict = compute_B_projections(
            bbasis,
            atomic_env_list,
            is_full=is_full,
            compute_forces_dict=True,
            verbose=verbose,
        )
        if verbose:
            log.info("B-projections computed:")
            for st, A0_proj in A0_proj_dict.items():
                log.info(
                    "\tElement: {}, B-projections shape: {}".format(
                        elements_name[st], A0_proj.shape
                    )
                )

        if not all_atoms:
            if verbose:
                log.info("Select atomic environments with force error below threshold")
            if error_based_threshold == 0:
                if verbose:
                    log.info("Automatic force error threshold determination(Q3+1.5*IQR)")
            else:
                if verbose:
                    log.info(f"Force error threshold: {error_based_threshold:.3f} eV/A")
            ref_forces_dict = extract_reference_forces_dict(
                df["ase_atoms"], df["forces"], elements_to_index_map
            )

            A0_projections_cropped_dict = {}
            sel_mask_dict = {}
            for symb, st in elements_to_index_map.items():
                dforces = ref_forces_dict[st] - forces_dict[st]
                dforces_norm = np.linalg.norm(dforces, axis=1)

                if error_based_threshold <= 0:
                    q1, q2, q3 = np.quantile(
                        dforces_norm,
                        q=[0.25, 0.5, 0.75],
                    )
                    iqr = q3 - q1
                    upper_bound = q3 + 1.5 * iqr
                else:
                    upper_bound = error_based_threshold

                sel_mask = dforces_norm <= upper_bound
                sel_mask_dict[st] = sel_mask

                f_rmse_before = np.sqrt(np.mean(dforces_norm**2))

                f_rmse_after = np.sqrt(np.mean(dforces_norm[sel_mask] ** 2))

                if verbose:
                    log.info(f"\tElement: {symb}, specie type: {st}")
                    log.info(f"\t\tForce error upper bound: {1e3 * upper_bound:.3f} meV/A")
                    log.info(
                        "\t\tNumber/fraction of selected atoms: {} ({:.1f}%)".format(
                            np.sum(sel_mask), 1e2 * np.mean(sel_mask)
                        )
                    )
                    log.info(f"\t\tForce RMSE (before): {1e3 * f_rmse_before:.3f} meV/A")
                    log.info(f"\t\tForce RMSE (after) : {1e3 * f_rmse_after:.3f} meV/A")
                A0_projections_cropped_dict[st] = A0_proj_dict[st][sel_mask]

            A0_proj_dict = A0_projections_cropped_dict

        if verbose:
            log.info("Compute active set (using MaxVol algorithm)")
        A_active_set_dict = compute_active_set(
            A0_proj_dict, tol=gamma_tolerance, max_iters=maxvol_iters, verbose=verbose
        )
        if verbose:
            log.info("Compute pseudoinversion of active set")
        A_active_inverse_set = compute_A_active_inverse(A_active_set_dict)
        if verbose:
            log.info("Done")
        gamma_dict = compute_extrapolation_grade(A0_proj_dict, A_active_inverse_set)
        gamma_max = {k: gg.max() for k, gg in gamma_dict.items()}

        if verbose:
            for st, AS_inv in A_active_inverse_set.items():
                log.info(
                    "\tElement: {}, Active set inv. shape: {}, gamma_max: {:.3f}".format(
                        elements_name[st], AS_inv.shape, gamma_max[st]
                    )
                )
            log.info(f"Saving Active Set Inversion (ASI) to {active_set_inv_filename}")
        with open(active_set_inv_filename, "wb") as f:
            np.savez(f, **{elements_name[st]: v for st, v in A_active_inverse_set.items()})
        if verbose:
            log.info(
                "Saving  done to {} ({})".format(
                    active_set_inv_filename, sizeof_fmt(active_set_inv_filename)
                )
            )
    else:
        # multiple round maxvol
        if verbose:
            log.info("Approximated MaxVol by batches")
            log.info(f"Batch size: {batch_size}")
        if not all_atoms:
            log.error(
                "WARNING! Error-based selection of atoms in batch mode is not implemented. Please use --all-atoms option"
            )
            sys.exit(1)
        n_batches = len(atomic_env_list) // batch_size
        if verbose:
            log.info(f"Number of batches: {n_batches}")

        if verbose:
            log.info("Compute approximate active set (using batched MaxVol algorithm)")
        (best_gamma, best_active_sets_dict, _) = compute_active_set_by_batches(
            bbasis,
            atomic_env_list=atomic_env_list,
            structure_ind_list=structure_ind_list,
            n_batches=n_batches,
            gamma_tolerance=gamma_tolerance,
            maxvol_iters=maxvol_iters,
            n_refinement_iter=maxvol_refinement,
            save_interim_active_set=True,
            is_full=is_full,
            verbose=verbose,
        )
        if verbose:
            log.info("Compute pseudoinversion of active set")
        A_active_inverse_set = compute_A_active_inverse(best_active_sets_dict)
        if verbose:
            for st, AS_inv in A_active_inverse_set.items():
                log.info(
                    "\tElement: {}, Active set inv. shape: {}, gamma_max: {:.3f}".format(
                        elements_name[st], AS_inv.shape, best_gamma[st]
                    )
                )
        if verbose:
            log.info(f"Saving Active Set Inversion (ASI) to {active_set_inv_filename}")
        save_active_inverse_set(
            active_set_inv_filename, A_active_inverse_set, elements_name=elements_name
        )
        if verbose:
            log.info(
                "Saving  done to {} ({})".format(
                    active_set_inv_filename, sizeof_fmt(active_set_inv_filename)
                )
            )


if __name__ == "__main__":
    main()
