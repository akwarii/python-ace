#!/usr/bin/env python
import argparse
import getpass
import glob
import importlib.resources
import logging
import os
import re
import shutil
import socket
import sys
from pathlib import Path
from shutil import copyfile

import pandas as pd
from ruamel.yaml import YAML

from pyace import BBasisConfiguration, __version__, get_ace_evaluator_version
from pyace.atomicenvironment import (
    calculate_minimal_nn_atomic_env,
    calculate_minimal_nn_tp_atoms,
)
from pyace.generalfit import GeneralACEFit
from pyace.preparedata import sizeof_fmt
from pyace.validate import plot_analyse_error_distributions

try:
    import readline

    no_readline = False
except ImportError:
    no_readline = True

hostname = socket.gethostname()
username = getpass.getuser()

LOG_FMT = "%(asctime)s %(levelname).1s - %(message)s"
logging.basicConfig(level=logging.INFO, format=LOG_FMT, datefmt="%Y/%m/%d %H:%M:%S")
logger = logging.getLogger()

FILES_TO_REMOVE = [
    "fitting_data_info.csv",
    "fitting_data_info.pckl.gzip",
    "test_data_info.pckl.gzip",
    "log.txt",
    "nohup.out",
    "target_potential.yaml",
    "current_extended_potential.yaml",
    "output_potential.yaml",
    "ladder_metrics.txt",
    "cycle_metrics.txt",
    "metrics.txt",
    "test_ladder_metrics.txt",
    "test_cycle_metrics.txt",
    "test_metrics.txt",
    "train_pred.pckl.gzip",
    "test_pred.pckl.gzip",
    "test_ef-distributions.png",
    "train_ef-distributions.png",
    "report",
]

DEFAULT_SEED = 42


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="pacemaker",
        description=f"Fitting utility for atomic cluster expansion potentials.\nversion: {__version__}",
    )
    parser.add_argument(
        "input",
        help="Path to the input YAML file (default: input.yaml)",
        nargs="?",
        type=str,
        default="input.yaml",
    )
    parser.add_argument(
        "-c",
        "--clean",
        help="Remove all generated data/files in the working directory",
        dest="clean",
        default=False,
        action="store_true",
    )
    parser.add_argument(
        "-o",
        "--output",
        help="Path to the output B-basis YAML file (default: output_potential.yaml)",
        default="output_potential.yaml",
        type=str,
    )
    parser.add_argument(
        "-p",
        "--potential",
        help="Path to the input potential YAML file, overrides 'potential' section in the input file",
        type=str,
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "-ip",
        "--initial-potential",
        help="Path to the initial potential YAML file, overrides 'potential::initial_potential' section in the input file",
        type=str,
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "-d",
        "--data",
        help="Path to the data file, overrides 'YAML:fit:filename' section in the input file",
        type=str,
        default=argparse.SUPPRESS,
    )
    parser.add_argument(
        "-l",
        "--log",
        help="Path to the log file (default: log.txt)",
        type=str,
        default="log.txt",
    )
    parser.add_argument(
        "-dr",
        "--dry-run",
        help="Perform all preprocessing and analysis steps without fitting the potential",
        dest="dry_run",
        action="store_true",
        default=False,
    )
    parser.add_argument(
        "-t",
        "--template",
        help="Generate a template 'input.yaml' file through an interactive dialog",
        dest="template",
        action="store_true",
        default=False,
    )
    parser.add_argument(
        "-v",
        "--version",
        help="Display version information and exit",
        dest="version",
        action="store_true",
        default=False,
    )
    parser.add_argument(
        "--no-fit",
        help="Skip the fitting process",
        dest="no_fit",
        action="store_true",
        default=False,
    )
    parser.add_argument(
        "--no-predict",
        help="Skip the prediction and saving of results",
        dest="no_predict",
        action="store_true",
        default=False,
    )
    parser.add_argument(
        "--verbose-tf",
        help="Enable verbose logging for TensorFlow (disabled by default)",
        dest="verbose_tf",
        action="store_true",
        default=False,
    )

    return parser


def setup_logging(log_file: str, verbose_tf: bool) -> None:
    if not verbose_tf:
        os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

    if log_file:
        logger.info(f"Redirecting log into file {log_file}")
        handler = logging.FileHandler(log_file, "a")
        formatter = logging.Formatter(LOG_FMT)
        handler.setFormatter(formatter)
        logger.addHandler(handler)


def clean_working_directory() -> None:
    print("Cleaning working directory. Removing files/folders:")

    interim_potentails = glob.glob("interim_potential*.yaml")
    ensemble_potentails = glob.glob("ensemble_potential*.yaml")
    to_delete = FILES_TO_REMOVE + interim_potentails + ensemble_potentails

    for fname in sorted(to_delete):
        fname = Path(fname)
        msg = f" - {fname}"
        if fname.is_file():
            fname.unlink()
        elif fname.is_dir():
            msg += " (folder)"
            shutil.rmtree(fname)
        print(msg)

    print("Done")


def load_yaml_config(filename: str) -> dict:
    yaml = YAML(typ="safe", pure=True)
    with open(filename) as f:
        data = yaml.load(f)
    return data


def generate_template_input() -> None:
    print("Generating 'input.yaml'")
    if not no_readline:
        readline.parse_and_bind("tab: complete")

    # 1. Training set size
    train_filename = input(
        "Enter training dataset filename (ex.: data.pckl.gzip, [TAB] - autocompletion): "
    )
    testset_size_inp = float(
        input("Enter test set fraction or size (ex.: 0.05 or [ENTER] - no test set): ") or 0.0
    )

    # 2. Elements
    determine_elements_from_dataset = False
    elements_str = input(
        "Please enter list of elements (ex.: 'Cu', 'AlNi', [ENTER] - determine from dataset): "
    )
    if elements_str:
        patt = re.compile("([A-Z][a-z]?)")
        elements = patt.findall(elements_str)
        elements = sorted(elements)
        determine_elements_from_dataset = False
    else:
        # determine from training set
        determine_elements_from_dataset = True

    # checking dataset
    print(f"Trying to load {train_filename}")
    df = pd.read_pickle(train_filename, compression="gzip")
    if determine_elements_from_dataset:
        if "ase_atoms" in df.columns:
            print("Determining available elements...")
            elements_set = set()
            df["ase_atoms"].map(lambda at: elements_set.update(at.get_chemical_symbols()))
            elements = sorted(elements_set)
            print("Found elements: ", elements)
        else:
            print("ERROR! No `ase_atoms` column found")
            sys.exit(1)
    if "energy_corrected" not in df.columns:
        print(f"`energy_corrected` column not found in dataset {train_filename}")
        resp = (
            input("Do you want to use the `energy` column instead? (yes/no/default-no): ") or "no"
        )
        if resp == "yes":
            df["energy_corrected"] = df["energy"]
            print(f"Saving updated dataset into {train_filename}...", end="")
            df.to_pickle(train_filename, compression="gzip")
            print("done")

    print("Number of elements: ", len(elements))
    print("Elements: ", elements)

    # number of functions per element
    number_of_functions_per_element = int(
        input("Enter number of functions per element ([ENTER] - default 700): ") or 700
    )
    print("Number of functions per element: ", number_of_functions_per_element)

    cutoff = float(input("Enter cutoff (Angstrom, default: 7.0): ") or 7.0)
    print("Cutoff: ", cutoff)

    # weighting scheme
    weighting = None
    while True:
        weighting_inp = (
            input("Enter weighting scheme type - `uniform` or `energy` ([ENTER] - `uniform`): ")
            or "uniform"
        )
        if weighting_inp in ["uniform", "energy"]:
            break
    if weighting_inp == "energy":
        weighting = "{ type: EnergyBasedWeightingPolicy, DElow: 1.0, DEup: 10.0, DFup: 50.0, DE: 1.0, DF: 1.0, wlow: 0.75, energy: convex_hull, reftype: all,seed: 42}"
        print("Use EnergyBasedWeightingPolicy: ", weighting)
    else:
        weighting = None
        print("Use UniformWeightingPolicy")

    template_input_yaml_filename = (
        importlib.resources.files("pyace") / "data" / "input_template.yaml"
    )
    with importlib.resources.as_file(template_input_yaml_filename) as path:
        copyfile(path, "input.yaml")

    with open("input.yaml") as f:
        input_yaml_text = f.read()

    input_yaml_text = input_yaml_text.replace("{{ELEMENTS}}", str(elements))
    input_yaml_text = input_yaml_text.replace("{{CUTOFF}}", str(cutoff))
    input_yaml_text = input_yaml_text.replace("{{DATAFILENAME}}", train_filename)
    input_yaml_text = input_yaml_text.replace(
        "{{number_of_functions_per_element}}",
        f"number_of_functions_per_element: {number_of_functions_per_element}",
    )
    if weighting:
        input_yaml_text = input_yaml_text.replace("{{WEIGHTING}}", "weighting: " + weighting)
    else:
        input_yaml_text = input_yaml_text.replace("{{WEIGHTING}}", "")

    if testset_size_inp > 0:
        input_yaml_text = input_yaml_text.replace(
            "{{test_size}}", f"test_size: {testset_size_inp}"
        )
    else:
        input_yaml_text = input_yaml_text.replace("{{test_size}}", "")

    with open("input.yaml", "w") as f:
        print(input_yaml_text, file=f)
    print("Input file is written into `input.yaml`")


def predict_and_save(
    general_fit: GeneralACEFit,
    target_bbasisconfig: BBasisConfiguration | None,
    structures_df: pd.DataFrame,
    fname: str,
) -> pd.DataFrame:
    pred_data = general_fit.predict(
        structures_dataframe=structures_df, bbasisconfig=target_bbasisconfig
    )
    if not isinstance(pred_data, pd.DataFrame):
        from pyace.const import ENERGY_PRED_COL, FORCES_PRED_COL

        pred_data = pd.DataFrame(
            {
                ENERGY_PRED_COL: pred_data[ENERGY_PRED_COL],
                FORCES_PRED_COL: pred_data[FORCES_PRED_COL],
            },
            index=structures_df.index,
        )

    if general_fit.evaluator_name == "pyace" and "atomic_env" in structures_df.columns:
        logger.info("Computing nearest neighbours distances from 'atomic_env'")
        structures_df["nn_min"] = structures_df["atomic_env"].map(calculate_minimal_nn_atomic_env)
    elif general_fit.evaluator_name == "tensorpot" and "tp_atoms" in structures_df.columns:
        logger.info("Computing nearest neighbours distances from 'tp_atoms'")
        structures_df["nn_min"] = structures_df["tp_atoms"].map(calculate_minimal_nn_tp_atoms)
    else:
        logger.error("No neighbour lists found, could not compute nearest neighbours distances")

    columns_to_drop = ["ase_atoms", "atomic_env", "tp_atoms"]
    columns_to_drop = [col for col in columns_to_drop if col in structures_df]
    pred_data = pd.merge(
        structures_df.drop(columns=columns_to_drop),
        pred_data,
        left_index=True,
        right_index=True,
    )
    pred_data.to_pickle(fname, compression="gzip", protocol=4)
    logger.info(f"Predictions are saved into {fname} ({sizeof_fmt(fname)})")
    return pred_data


def make_predictions(
    args: argparse.Namespace, general_fit: GeneralACEFit, backend_config: dict
) -> None:
    logger.info("Making predictions")

    # if fit was not done - just take target_bbasisconfig
    target_bbasisconfig = None
    if args.no_fit:
        target_bbasisconfig = general_fit.target_bbasisconfig

    if general_fit.fitting_data is not None:
        logger.info("For train data")
        pred_data = predict_and_save(
            general_fit,
            target_bbasisconfig,
            general_fit.fitting_data,
            fname="train_pred.pckl.gzip",
        )
        logger.info("Plotting validation graphs")
        plot_analyse_error_distributions(
            pred_data,
            fig_prefix="train_",
            fig_path="report",
            imagetype=backend_config.get("imagetype", "png"),
        )

    if general_fit.test_data is not None:
        logger.info("For test data")
        pred_data = predict_and_save(
            general_fit,
            target_bbasisconfig,
            general_fit.test_data,
            fname="test_pred.pckl.gzip",
        )
        logger.info("Plotting validation graphs")
        plot_analyse_error_distributions(
            pred_data,
            fig_prefix="test_",
            fig_path="report",
            imagetype=backend_config.get("imagetype", "png"),
        )


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    if args.version:
        print(f"pacemaker/pyace version: {__version__}")
        print(f"ace_evaluator   version: {get_ace_evaluator_version()}")
        sys.exit(0)

    if args.clean:
        clean_working_directory()
        sys.exit(0)

    if args.template:
        generate_template_input()
        sys.exit(0)

    setup_logging(args.log, args.verbose_tf)

    if args.dry_run:
        logger.info("====== DRY RUN ======")

    logger.info("Start pacemaker")
    logger.info(f"Hostname: {hostname}")
    logger.info(f"Username: {username}")
    logger.info(f"pacemaker/pyace version: {__version__}")
    logger.info(f"ace_evaluator   version: {get_ace_evaluator_version()}")
    logger.info(f"Loading {args.input}... ")

    yaml_config = load_yaml_config(args.input)

    # ??? unsure about why this is checked since cutoff is never used
    if "cutoff" not in yaml_config:
        logger.error("No 'cutoff' provided in YAML file, please specify it")
        raise ValueError("No 'cutoff' provided in YAML file, please specify it")

    # seed section
    seed = yaml_config.get("seed", DEFAULT_SEED)
    if "seed" not in yaml_config:
        logger.warning(
            f"No 'seed' provided in YAML file, default value seed = {DEFAULT_SEED} will be used."
        )

    # data section
    if "data" in args:
        data_config = {"filename": args.data}
        logger.info(f"Overwrite 'data' with {data_config}")
    elif "data" in yaml_config:
        data_config = yaml_config["data"]
    else:
        raise ValueError("'data' section is not provided neither in input file nor in arguments")

    env_data_path = os.environ.get("PACEMAKERDATAPATH")
    if env_data_path is not None and not data_config.get("datapath"):
        data_config["datapath"] = env_data_path
        logger.info(f"Data path set to $PACEMAKERDATAPATH = {env_data_path}")

    # backend section
    backend_config = yaml_config.get("backend", {"evaluator": "pyace", "parallel_mode": "process"})
    if "backend" not in yaml_config:
        logger.warning(
            f"'backend' is not specified, default settings will be used: {backend_config}"
        )

    evaluator_name = backend_config.get("evaluator", "pyace")
    if "evaluator" not in backend_config:
        backend_config["evaluator"] = "pyace"
        logger.info("Couldn't find evaluator ('pyace' or 'tensorpot').")
        logger.info(
            f"Default evaluator `{evaluator_name}` would be used, otherwise please specify in YAML:backend:evaluator"
        )

    # potential section
    if "potential" in args:
        potential_config = args.potential
        logger.info(f"Potential settings is overwritten from arguments: {potential_config}")
    elif "potential" in yaml_config:
        potential_config = yaml_config["potential"]
        if isinstance(potential_config, dict):
            if "metadata" in yaml_config:
                potential_config["metadata"] = yaml_config["metadata"]
    else:
        raise ValueError("'potential' section is not given")

    if "initial_potential" in args:
        if isinstance(potential_config, dict):
            potential_config["initial_potential"] = args.initial_potential
        else:
            raise ValueError(
                "Couldn't combine `initial_potential` setting with non-dictionary `potential` setting"
            )

    # fit section
    fit_config = yaml_config.get("fit", dict())
    callbacks = fit_config.get("callbacks", [])

    general_fit = GeneralACEFit(
        potential_config=potential_config,
        fit_config=fit_config,
        data_config=data_config,
        backend_config=backend_config,
        seed=seed,
        callbacks=callbacks,
    )

    if args.dry_run:
        logger.info("Dry run is finished")
        sys.exit(0)

    if not args.no_fit:
        general_fit.fit()
        general_fit.set_core_rep(general_fit.target_bbasisconfig)
        general_fit.save_optimized_potential(args.output)

    if not args.no_predict:
        make_predictions(args, general_fit, backend_config)


if __name__ == "__main__":
    main()
