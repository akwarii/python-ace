#!/usr/bin/env python
import argparse
import logging
import sys
from collections import defaultdict
from pathlib import Path

import numpy as np
import pandas as pd

from pyace import BBasisConfiguration, PyACECalculator
from pyace.aceselect import load_multiple_datasets
from pyace.atomicenvironment import aseatoms_to_atomicenvironment
from pyace.data_aug import compute_enn_df, select_reliable_enn_part
from pyace.generalfit import setup_inner_core_repulsion, setup_zbl_inner_core_repulsion
from pyace.process_df import tqdm
from pyace.utils.utils import complement_min_dist_dict, compute_nn_dist_per_bond

LOG_FMT = "%(asctime)s %(levelname).1s - %(message)s"
logging.basicConfig(level=logging.INFO, format=LOG_FMT, datefmt="%Y/%m/%d %H:%M:%S")
log = logging.getLogger()


class InvalidArgumentError(ValueError):
    pass


class ParseKwargs(argparse.Action):
    def __call__(
        self,
        parser: argparse.ArgumentParser,
        namespace: argparse.Namespace,
        values: list[str],
        option_string: str | None = None,
    ) -> None:
        setattr(namespace, self.dest, dict())
        for value in values:
            if value == "auto":
                key = "auto"
                val = True
            else:
                splt = value.split(":")
                key = splt[0]

                try:
                    val = float(splt[1])
                except ValueError:
                    if len(splt) != 2 or splt[1] != "auto":
                        raise InvalidArgumentError(
                            f"Invalid argument value for free atom energy: {value}. "
                            "Should be in the format: 'element:cutoff' or 'element:auto'."
                        )
                    val = splt[1]

            getattr(namespace, self.dest)[key] = val


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        prog="pace_corerep",
        description="Utility to (auto)tune potential and add ZBL core-repulsion ZBL",
        add_help=False,
    )
    required = parser.add_argument_group("required named arguments")
    optional = parser.add_argument_group("optional named arguments")
    parser.add_argument(
        "potential_file",
        help="Path to the B-basis file (.yaml) containing the potential.",
        type=str,
    )
    required.add_argument(
        "-d",
        "--dataset",
        type=str,
        nargs="+",
        required=True,
        help="Path(s) to dataset file(s). Example: -d filename.pckl.gzip [-d filename2.pckl.gzip].",
    )
    required.add_argument(
        "-a",
        "--active-set-inv",
        help="Path to the Active Set Inverted (ASI) file, used as extra B-projections.",
        default=None,
        type=str,
        dest="active_set_inv_fname",
        required=True,
    )
    optional.add_argument(
        "-h",
        "--help",
        action="help",
        help="Show this help message and exit",
    )
    optional.add_argument(
        "-o",
        "--output",
        help="Output filename for the auto-tuned core-rep potential. "
        "If not specified, the same as `potential_file`. If set to `auto`, a 'corerep' suffix will be added.",
        default=None,
        type=str,
        dest="output_file",
    )
    optional.add_argument(
        "-V",
        help="Suppress verbosity of numerical procedures.",
        dest="not_verbose",
        default=False,
        action="store_true",
    )
    optional.add_argument(
        "-nnstep",
        "--nn-dist-step",
        type=float,
        dest="nn_distance_step",
        help="Step size for nearest-neighbour distance in data augmentation (default = 0.05).",
        default=0.05,
    )
    optional.add_argument(
        "-nnmin",
        "--nn-dist-min",
        type=float,
        dest="nn_distance_min",
        help="Minimum nearest-neighbour distance for data augmentation (default = 0.5).",
        default=0.5,
    )
    optional.add_argument(
        "-nnmax",
        "--nn-dist-max",
        type=float,
        dest="nn_distance_max",
        help="Maximum nearest-neighbour distance for data augmentation (default = 2.5).",
        default=2.5,
    )
    optional.add_argument(
        "-n",
        "--num-of-structures",
        type=int,
        dest="num_of_structures",
        help="Number of structures to select for compression (default = 50).",
        default=50,
    )
    optional.add_argument(
        "-g",
        "--gamma-max",
        type=float,
        dest="gamma_max",
        help="Maximum extrapolation grade gamma for reliable atomic environments (default = 10).",
        default=10,
    )
    optional.add_argument(
        "--inner-cutoff",
        help="Dictionary of inner cutoffs in the format `Al:1.5 Cu-Cu:1.6 Al-Cu:1.55`. Default is zero.",
        nargs="*",
        dest="inner_cutoff_dict",
        default=defaultdict(int),
        action=ParseKwargs,
    )

    return parser


def save_potential(bconf_orig_norep, output_file, verbose):
    bconf_orig_norep.save(output_file)
    if verbose:
        log.info(f"Auto-tuned potential is saved into {output_file}")


def process_user_defined_cutoff(
    cutoff_dict: dict[str, str],
    bconf_orig_norep: BBasisConfiguration,
    verbose: bool,
) -> dict[tuple[int, int], float]:
    calc = PyACECalculator(bconf_orig_norep)
    elements_to_index_map = calc.basis.elements_to_index_map
    elements = calc.basis.elements_name

    if verbose:
        cutoff_str = ", ".join([f"{k}:{v}" for k, v in cutoff_dict.items()])
        log.info(f"Inner cutoff options: {cutoff_str}")

    min_dist_per_bond_dict = defaultdict(float)
    for key, value in cutoff_dict.items():
        r_in = float(value)

        splitted_key = key.split("-")
        if len(splitted_key) == 1:
            el0 = elements_to_index_map[splitted_key[0]]
            el1 = el0
        elif len(splitted_key) == 2:
            el0 = elements_to_index_map[splitted_key[0]]
            el1 = elements_to_index_map[splitted_key[1]]
        else:
            raise ValueError(f"Can't process user-defined cutoffs: {key}:{r_in}")

        bond = tuple(sorted([el0, el1]))
        min_dist_per_bond_dict[bond] = r_in

    if verbose:
        log.info("User-defined inner cutoff: ")
        for k, v in min_dist_per_bond_dict.items():
            log.info(f" {elements[k[0]]}-{elements[k[1]]}: {v:.3f} Ang")

    return min_dist_per_bond_dict


def min_dist_per_bond_type(
    df: pd.DataFrame,
    calc: PyACECalculator,
) -> dict[tuple[int, int], float]:
    elements_mapper_dict = calc.basis.elements_to_index_map
    df["nn_dist_per_bond"] = df["ase_atoms"].apply(
        compute_nn_dist_per_bond,
        cutoff=calc.basis.cutoffmax,
        elements_mapper_dict=elements_mapper_dict,
    )

    # Build a dictionary of minimal distances per bond type
    min_nn_dist_dd = defaultdict(list)
    for nn_dist_per_bond in df["nn_dist_per_bond"]:
        for k, v in nn_dist_per_bond.items():
            min_nn_dist_dd[k].append(v)

    log.info("Minimal distance threshold (quantile=0.001) per bond (Ang):")
    bond_quantile_dict = {}
    for k, v in min_nn_dist_dd.items():
        q_th = 0.001
        q = np.quantile(v, q=q_th)
        bond_quantile_dict[k] = q
        log.info(f"Bond {k}:  {q:.3f} (#{len(v)} nn_dists)")

    return bond_quantile_dict


def build_candidate_list(
    df: pd.DataFrame,
    structure_limit: int,
) -> pd.Series:
    df["NUMBER_OF_ATOMS"] = df["ase_atoms"].map(len)
    nat_quant50 = int(df["NUMBER_OF_ATOMS"].quantile(q=0.5))
    candidate_list = df.query(f"NUMBER_OF_ATOMS<={nat_quant50}").sort_values("NUMBER_OF_ATOMS")[
        "ase_atoms"
    ]
    candidate_list = candidate_list.sample(
        n=min(structure_limit, len(candidate_list)), random_state=42
    )

    return candidate_list


def generate_bond_distance_map(
    df: pd.DataFrame,
    candidates: pd.Series,
    calc: PyACECalculator,
    min_nn_dist: float,
    max_nn_dist: float,
    nn_dist_step: float,
    gamma_max: float,
    verbose: bool,
) -> dict:
    # TODO check what can possibly fail here
    nn_distance_range = (min_nn_dist, max_nn_dist)
    map_bond_dist = defaultdict(list)
    for at in tqdm(candidates):
        try:
            enndf = compute_enn_df(
                at,
                calc=calc,
                compute_zbl=True,
                compute_gamma=True,
                nn_distance_range=nn_distance_range,
                nn_distance_step=nn_dist_step,
            )

            rel_kink_df = select_reliable_enn_part(enndf, reliability_criteria="kink")
            rel_gamma_df = select_reliable_enn_part(
                enndf, reliability_criteria="extrapolation", gamma_max=gamma_max
            )

            enndf_zbl = enndf.query("epa<=epa_zbl")

            rel_z = min(rel_kink_df["z"].min(), rel_gamma_df["z"].min())
            rel_z = max(enndf_zbl["z"].min(), rel_z)
            enndf = enndf[enndf["z"] <= rel_z]

            cur_map_bond_dist = defaultdict(list)
            for _, row in enndf.iterrows():
                at = row["ase_atoms"]
                ae = aseatoms_to_atomicenvironment(
                    at,
                    cutoff=calc.basis.cutoffmax,
                    elements_mapper_dict=calc.basis.elements_to_index_map,
                )

                gamma_per_atom = row["gamma_per_atom"]
                for gamma, (mu_i, mu_j, dist) in zip(
                    gamma_per_atom, ae.get_nearest_atom_type_and_distance()
                ):
                    if gamma >= gamma_max:
                        cur_map_bond_dist[(mu_i, mu_j)].append(dist)

            for k, v in cur_map_bond_dist.items():
                map_bond_dist[k].append(max(v))

        except Exception as e:
            log.error(f"ERROR for structure {at.get_chemical_formula()}:", e)

    min_dist_per_bond_dict = defaultdict(float)
    for k, v in map_bond_dist.items():
        r_in = np.quantile(v, q=0.99)
        k = tuple(sorted(k))
        min_dist_per_bond_dict[k] = max(min_dist_per_bond_dict[k], r_in)  # type: ignore

    # TODO: fill remaining bond pairs with min_dist
    elements = calc.basis.elements_name
    bond_quantile_dict = min_dist_per_bond_type(df, calc)
    min_dist_per_bond_dict = complement_min_dist_dict(
        min_dist_per_bond_dict, bond_quantile_dict, elements, verbose
    )

    return min_dist_per_bond_dict


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    potential_file = args.potential_file
    output_file = args.output_file
    verbose = not args.not_verbose
    active_set_file = args.active_set_inv_fname
    inner_cutoff_dict = args.inner_cutoff_dict

    if verbose:
        logging.info(f"Loading B-basis potential from {potential_file}")
    bconf_orig_norep = BBasisConfiguration(potential_file)

    if output_file is None:
        output_file = potential_file
    elif output_file == "auto":
        output_file = potential_file.replace(".yaml", "-corerep.yaml")

    # User-defined inner cutoff
    if len(inner_cutoff_dict) > 0:
        min_dist_per_bond_dict = process_user_defined_cutoff(
            inner_cutoff_dict, bconf_orig_norep, verbose
        )
        setup_zbl_inner_core_repulsion(bconf_orig_norep, min_dist_per_bond_dict)
        save_potential(bconf_orig_norep, output_file, verbose)
        sys.exit(0)

    # Remove core repulsion
    # ??? Why do we need to remove core repulsion here and not for user defined cutoffs?
    if verbose:
        log.info("Removing existing core repulsion from the potential")

    setup_inner_core_repulsion(bconf_orig_norep, r_in=0, delta_in=0, core_rep_parameters=(0, 1))
    calc = PyACECalculator(bconf_orig_norep)

    # Load active set
    if verbose:
        log.info("Loading active set")

    if active_set_file is None:
        active_set_file = potential_file.replace(".yaml", ".asi")
        if not Path(active_set_file).is_file():
            raise ValueError(
                f"Active set file {active_set_file} neither set nor found, "
                + "but it is required for correct core-rep autotune. Please generate it if missing or provide it explicitly."
            )

    calc.set_active_set(active_set_file)

    # Load dataset
    # TODO currently I think dataset can't be a list, need to check
    dataset_filename = args.dataset
    if isinstance(dataset_filename, list):
        df = load_multiple_datasets(dataset_filename, verbose=verbose)

        if verbose:
            log.info(f"{len(df['ase_atoms'])} structures in candidate list")

        # Drop augmented structures
        if "name" in df.columns:
            size_before = len(df)
            df = df.query("not name.str.startswith('augmented')").reset_index(drop=True)
            ndrop = size_before - len(df)

            if verbose and ndrop > 0:
                log.info(
                    f"AUGMENTED STRUCTURES: {ndrop} structure(s) were dropped, new dataset size: {len(df)}"
                )
    else:
        raise ValueError(f"Unrecognized --dataset (-d) argument: {dataset_filename}")

    candidate_list = build_candidate_list(df, args.num_of_structures)
    if verbose:
        max_at = max(map(len, candidate_list))
        log.info(
            f"Candidate list contains {len(candidate_list)} structures with up-to {max_at} atoms"
        )

    min_dist_per_bond_dict = generate_bond_distance_map(
        df=df,
        candidates=candidate_list,
        calc=calc,
        min_nn_dist=args.nn_distance_min,
        max_nn_dist=args.nn_distance_max,
        nn_dist_step=args.nn_distance_step,
        gamma_max=args.gamma_max,
        verbose=verbose,
    )

    elements = calc.basis.elements_name
    if verbose:
        log.info("Calculated inner cutoff (99% percentile or minimal distance): ")
        for k, v in min_dist_per_bond_dict.items():
            log.info(f" {elements[k[0]]}-{elements[k[1]]}: {v:.3f} Ang")

    setup_zbl_inner_core_repulsion(bconf_orig_norep, min_dist_per_bond_dict)
    save_potential(bconf_orig_norep, output_file, verbose)


if __name__ == "__main__":
    main()
